{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "import os\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import dash\n",
    "import itertools\n",
    "import statsmodels.api as sm\n",
    "import re\n",
    "import scikit_posthocs as sp\n",
    "import seaborn as sns\n",
    "from statsmodels.formula.api import ols\n",
    "from matplotlib import pyplot\n",
    "from sklearn.decomposition import PCA\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from itertools import product\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from dash import html\n",
    "from dash import dcc\n",
    "from scipy.stats import chi2\n",
    "from scipy.stats import kruskal\n",
    "from statsmodels.graphics.factorplots import interaction_plot\n",
    "from scipy.stats import shapiro\n",
    "from scipy.stats import levene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All Data is available from the Git Repository at https://github.com/paulr28/Msc_Capstone.git unless otherwise stated.\n",
    "If the data is not available from the repository, a transformed version of the data will be available to continue the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Delay Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Wait Assessment 2020 Onwards - \"https://data.ny.gov/resource/swky-c3v4.csv\"\n",
    "\n",
    "# Create a directory to store the csv files\n",
    "os.makedirs(\"data\", exist_ok=True)\n",
    "\n",
    "# Download the csv file\n",
    "wait_assessment_2020 = \"https://data.ny.gov/resource/swky-c3v4.csv?$limit=1000000000\"\n",
    "response = requests.get(wait_assessment_2020)\n",
    "\n",
    "# Save the csv file to disk\n",
    "csv_file = \"wait_assessment_2020.csv\"\n",
    "with open(csv_file, \"w\") as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "#Save the csv file to a dataframe    \n",
    "wait_assessment_2020 = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column and row count\n",
    "print(wait_assessment_2020.shape)\n",
    "\n",
    "# Describe the dataframe\n",
    "wait_assessment_2020.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column name and data type\n",
    "print(wait_assessment_2020.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Split the month and year into separate columns\n",
    "wait_assessment_2020[[\"Year\", \"Month\"]] = wait_assessment_2020[\"month\"].str.split(\"-\", expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the dataframe\n",
    "wait_assessment_2020.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Wait Assessment 2015 - 2019 - \"https://data.ny.gov/resource/bmix-dpzc.csv\"\n",
    "\n",
    "# Download the csv file\n",
    "wait_assessment_2015 = \"https://data.ny.gov/resource/bmix-dpzc.csv?$limit=1000000000\"\n",
    "response = requests.get(wait_assessment_2015)\n",
    "\n",
    "# Save the csv file to disk\n",
    "csv_file = \"wait_assessment_2015.csv\"\n",
    "with open(csv_file, \"w\") as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "#Save the csv file to a dataframe    \n",
    "wait_assessment_2015 = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the dataframe\n",
    "print(wait_assessment_2015.shape)\n",
    "\n",
    "# Describe the dataframe\n",
    "wait_assessment_2015.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Split the month and year into separate columns\n",
    "wait_assessment_2015[[\"Year\", \"Month\"]] = wait_assessment_2015[\"month\"].str.split(\"-\", expand=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Merge the two dataframes\n",
    "wait_assessment_2015_2020 = pd.concat([wait_assessment_2020, wait_assessment_2015], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the dataframe\n",
    "print(wait_assessment_2015_2020.shape)\n",
    "\n",
    "# Describe the dataframe\n",
    "wait_assessment_2015_2020.describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service Deliviered 2020 Onwards - \"https://data.ny.gov/resource/2e6s-9gpm.csv\"\n",
    "\n",
    "# Download the csv file\n",
    "service_delivered_2020 = \"https://data.ny.gov/resource/2e6s-9gpm.csv?$limit=1000000000\"\n",
    "response = requests.get(service_delivered_2020)\n",
    "\n",
    "# Save the csv file to disk\n",
    "csv_file = \"service_delivered_2020.csv\"\n",
    "with open(csv_file, \"w\") as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "#Save the csv file to a dataframe    \n",
    "service_delivered_2020 = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Service Deliviered 2015 - 2019 - \"https://data.ny.gov/resource/tw28-zvtk.csv\"\n",
    "\n",
    "# Download the csv file\n",
    "service_delivered_2015 = \"https://data.ny.gov/resource/tw28-zvtk.csv?$limit=1000000000\"\n",
    "response = requests.get(service_delivered_2015)\n",
    "\n",
    "# Save the csv file to disk\n",
    "csv_file = \"service_delivered_2015.csv\"\n",
    "with open(csv_file, \"w\") as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "#Save the csv file to a dataframe    \n",
    "service_delivered_2015 = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Traffic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a large file and takes some time to download.  It is not included in the repository. but the transformed data is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hourly Traffic on (MTA) Bridges and Tunnels: Beginning 2010 - \"https://data.ny.gov/resource/qzve-kjga.csv\"\n",
    "\n",
    "# Download the csv file\n",
    "hourly_traffic = \"https://data.ny.gov/resource/qzve-kjga.csv?$limit=1000000000\"\n",
    "response = requests.get(hourly_traffic)\n",
    "\n",
    "# Save the csv file to disk\n",
    "csv_file = \"hourly_traffic.csv\"\n",
    "with open(csv_file, \"w\") as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "#Save the csv file to a dataframe    \n",
    "hourly_traffic = pd.read_csv(csv_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Loading the csv file for hourly_traffic\n",
    "hourly_traffic = pd.read_csv('hourly_traffic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the dataframe\n",
    "print(hourly_traffic.shape)\n",
    "\n",
    "# Describe the dataframe\n",
    "hourly_traffic.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Create a new column named total_vehicles by adding the vehicles_e_zpass and vehicles_vtoll columns\n",
    "hourly_traffic[\"total_vehicles\"] = hourly_traffic[\"vehicles_e_zpass\"] + hourly_traffic[\"vehicles_vtoll\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Extract the year from the date_time column\n",
    "hourly_traffic[\"year\"] = hourly_traffic[\"date\"].str[:4]\n",
    "\n",
    "# Extract the month from the date column\n",
    "hourly_traffic[\"month\"] = hourly_traffic[\"date\"].str[5:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Transform the dataframe to group by the Year, Month, plaza_id, and direction columns, giving the sum of the total_vehicles, vehicles_e_zpass, and vehicles_vtoll columns\n",
    "hourly_traffic_transformed = hourly_traffic.groupby([\"year\", \"month\", \"plaza_id\", \"direction\"]).agg({\"total_vehicles\": \"sum\", \"vehicles_e_zpass\": \"sum\", \"vehicles_vtoll\": \"sum\"}).reset_index()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the dataframe\n",
    "print(hourly_traffic_transformed.shape)\n",
    "\n",
    "# Describe the dataframe\n",
    "hourly_traffic_transformed.describe(include=\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column named 'value_type', all the values in this column should say 'vehicle_count'\n",
    "hourly_traffic_transformed = hourly_traffic_transformed.assign(value_type=\"vehicle_count\")\n",
    "\n",
    "# change the plaza_id column to location_id, and change total_vehicles to value\n",
    "hourly_traffic_transformed = hourly_traffic_transformed.rename(columns={\"plaza_id\": \"location_id\", \"total_vehicles\": \"value\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep columns 1-5 and column 8 and drop the rest\n",
    "hourly_traffic_transformed = hourly_traffic_transformed.iloc[:, list(range(5)) + [7]]\n",
    "\n",
    "# swap the order of the last 2 columns\n",
    "hourly_traffic_transformed = hourly_traffic_transformed.iloc[:, [0, 1, 2, 3, 5, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column named 'data_source', all the values in this column should say 'hourly_traffic'\n",
    "hourly_traffic_transformed = hourly_traffic_transformed.assign(data_source=\"hourly_traffic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to a csv file\n",
    "hourly_traffic_transformed.to_csv(\"hourly_traffic_transformed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Load the csv file for hourly_traffic_transformed\n",
    "hourly_traffic_transformed = pd.read_csv(\"hourly_traffic_transformed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hourly_traffic_transformed is one traffic df\n",
    "columns are:\n",
    "year\n",
    "month\n",
    "location_id\n",
    "direction\n",
    "value_type\n",
    "value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following csv is a large file available at https://data.cityofnewyork.us/resource/7ym2-wayt\n",
    "The full file has been downloaded and is used in the following script.\n",
    "The aggregate output will also be made available and it is recommended this is the version to work with.\n",
    "It was too large to upload to the repository.\n",
    "To use the transformed data skip to cell 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a CSV file and write it to a df using pandas\n",
    "# Open the CSV file\n",
    "\n",
    "traffic_speeds = pd.read_csv('DOT_Traffic_Speeds_NBE_20240121.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_backup = traffic_speeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the dataframe\n",
    "print(traffic_speeds.shape)\n",
    "\n",
    "# Describe the dataframe\n",
    "traffic_speeds.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep the Speed, travel_time, status, data_as_of, link_id, and borough columns\n",
    "traffic_speeds = traffic_speeds[['SPEED', 'TRAVEL_TIME', 'STATUS', 'DATA_AS_OF', 'LINK_ID', 'BOROUGH']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year from the data_as_of column which is in the format of 09/20/2019 07:27:03 AM\n",
    "traffic_speeds[\"year\"] = traffic_speeds[\"DATA_AS_OF\"].str[6:10]\n",
    "\n",
    "# Extract the month from the data_as_of column which is in the format of 09/20/2019 07:27:03 AM\n",
    "traffic_speeds[\"month\"] = traffic_speeds[\"DATA_AS_OF\"].str[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by the Year, Month, and BOROUGH and LINK_ID, columns, giving the mean of the speed and travel_time columns\n",
    "traffic_speeds_transformed = traffic_speeds.groupby([\"year\", \"month\", \"BOROUGH\", \"LINK_ID\"]).agg({\"SPEED\": \"mean\", \"TRAVEL_TIME\": \"mean\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the dataframe\n",
    "print(traffic_speeds_transformed.shape)\n",
    "\n",
    "# Describe the dataframe\n",
    "traffic_speeds_transformed.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file\n",
    "traffic_speeds_transformed.to_csv(\"traffic_speeds_transformed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# load the transformed dataframe from a csv file\n",
    "traffic_speeds_transformed = pd.read_csv(\"traffic_speeds_transformed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the dataframe so that the values in the speed and travel_time columns are in the same column and the column names are in a column named 'value_type'\n",
    "traffic_speeds_transformed = traffic_speeds_transformed.melt(id_vars=[\"year\", \"month\", \"BOROUGH\", \"LINK_ID\"], value_vars=[\"SPEED\", \"TRAVEL_TIME\"], var_name=\"value_type\", value_name=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase the value_type column\n",
    "traffic_speeds_transformed[\"value_type\"] = traffic_speeds_transformed[\"value_type\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the BOROUGH column and change the LINK_ID column to location_id\n",
    "traffic_speeds_transformed = traffic_speeds_transformed.drop(columns=[\"BOROUGH\"])\n",
    "traffic_speeds_transformed = traffic_speeds_transformed.rename(columns={\"LINK_ID\": \"location_id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column named 'direction' with all the values in this column should be null\n",
    "traffic_speeds_transformed = traffic_speeds_transformed.assign(direction=\"null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move the direction column to behind the location_id column\n",
    "traffic_speeds_transformed = traffic_speeds_transformed.iloc[:, [0, 1, 2, 5, 3, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column named 'data_source' with all the values in this column should be 'traffic_speeds'\n",
    "traffic_speeds_transformed = traffic_speeds_transformed.assign(data_source=\"traffic_speeds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file\n",
    "traffic_speeds_transformed.to_csv(\"traffic_speeds_transformed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# load the transformed dataframe from a csv file\n",
    "traffic_speeds_transformed = pd.read_csv(\"traffic_speeds_transformed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "traffic_speeds_transformed is one traffic df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip download and use the transformed data instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://data.cityofnewyork.us/Transportation/Automated-Traffic-Volume-Counts/7ym2-wayt\n",
    "\n",
    "# Download the csv file\n",
    "# Use a select and group by statement to get the count of the volume column grouped by the year, month, and roadway_name columns\n",
    "\n",
    "automated_traffic_volume_counts = \"https://data.ny.gov/resource/7ym2-wayt.csv?$select=Yr,M,Boro,sum(Vol)&$group=Yr,M,Boro&$limit=1000000000\" \n",
    "response = requests.get(automated_traffic_volume_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the csv file to disk\n",
    "csv_file = \"automated_traffic_volume_counts.csv\"\n",
    "with open(csv_file, \"w\") as f:\n",
    "    f.write(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Load the csv file to a dataframe\n",
    "automated_traffic_volume_counts = pd.read_csv(\"automated_traffic_volume_counts.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automated_traffic_volume_counts.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the column names to year, month, location_id, and value\n",
    "automated_traffic_volume_counts = automated_traffic_volume_counts.rename(columns={\"Yr\": \"year\", \"M\": \"month\", \"Boro\": \"location_id\", \"sum_Vol\": \"value\"})\n",
    "\n",
    "# Add a column named 'value_type' with all the values in this column should say 'vehicle_count'\n",
    "automated_traffic_volume_counts = automated_traffic_volume_counts.assign(value_type=\"vehicle_count\")\n",
    "\n",
    "# Add a column named 'direction' with all the values in this column should be null\n",
    "automated_traffic_volume_counts = automated_traffic_volume_counts.assign(direction=\"null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the order of the columns to 0,1,2,5,4,3\n",
    "automated_traffic_volume_counts = automated_traffic_volume_counts.iloc[:, [0, 1, 2, 5, 4, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column named 'data_source' with all the values in this column should be 'automated_traffic_volume_counts'\n",
    "automated_traffic_volume_counts = automated_traffic_volume_counts.assign(data_source=\"automated_traffic_volume_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file\n",
    "automated_traffic_volume_counts.to_csv(\"automated_traffic_volume_counts_transformed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "automated_traffic_volume_counts_transformed = pd.read_csv(\"automated_traffic_volume_counts_transformed.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "automated_traffic_volume_counts_transformed is one traffic df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Daily Traffic - \"https://data.ny.gov/resource/cwhc-n4ek.csv\"\n",
    "\n",
    "# Download the csv file\n",
    "daily_traffic = \"https://data.ny.gov/resource/cwhc-n4ek.csv?$limit=1000000000\"\n",
    "response = requests.get(daily_traffic)\n",
    "\n",
    "# Save the csv file to disk\n",
    "csv_file = \"daily_traffic.csv\"\n",
    "with open(csv_file, \"w\") as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "#Save the csv file to a dataframe    \n",
    "daily_traffic = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Load the csv file\n",
    "daily_traffic = pd.read_csv('daily_traffic.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the dataframe\n",
    "print(daily_traffic.shape)\n",
    "\n",
    "# Describe the dataframe\n",
    "daily_traffic.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the year from the date column\n",
    "daily_traffic[\"year\"] = daily_traffic[\"date\"].str[:4]\n",
    "\n",
    "# Extract the month from the date column\n",
    "daily_traffic[\"month\"] = daily_traffic[\"date\"].str[5:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the vehicles_e_zpass and vehicles_vtoll columns to get the total_vehicles column\n",
    "daily_traffic[\"value\"] = daily_traffic[\"vehicles_e_zpass\"] + daily_traffic[\"vehicles_vtoll\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sum of the total_vehicles, vehicles_e_zpass, and vehicles_vtoll columns grouped by the Year, Month, plaza_id, and direction columns\n",
    "daily_traffic_transformed = daily_traffic.groupby([\"year\", \"month\", \"plaza_id\", \"direction\"]).agg({\"value\": \"sum\", \"vehicles_e_zpass\": \"sum\", \"vehicles_vtoll\": \"sum\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the dataframe to a csv file\n",
    "daily_traffic_transformed.to_csv(\"daily_traffic_transformed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Load the csv file\n",
    "daily_traffic_transformed = pd.read_csv('daily_traffic_transformed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the dataframe\n",
    "print(daily_traffic_transformed.shape)\n",
    "\n",
    "# Describe the dataframe\n",
    "daily_traffic_transformed.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column named 'value_type' with all the values in this column should say 'vehicle_count'\n",
    "daily_traffic_transformed = daily_traffic_transformed.assign(value_type=\"vehicle_count\")\n",
    "\n",
    "# change the plaza_id column to location_id\n",
    "daily_traffic_transformed = daily_traffic_transformed.rename(columns={\"plaza_id\": \"location_id\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# swap the order of the last 2 columns\n",
    "daily_traffic_transformed = daily_traffic_transformed.iloc[:, [0, 1, 2, 3, 7, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a column named 'data_source' with all the values in this column should be 'daily_traffic'\n",
    "daily_traffic_transformed = daily_traffic_transformed.assign(data_source=\"daily_traffic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file\n",
    "daily_traffic_transformed.to_csv(\"daily_traffic_transformed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# load the transformed dataframe from a csv file\n",
    "daily_traffic_transformed = pd.read_csv(\"daily_traffic_transformed.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "daily_traffic_transformed is one traffic df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traffic Volume Count Smaller - \"https://data.cityofnewyork.us/resource/btm5-ppia.csv\"\n",
    "\n",
    "# Download the csv file\n",
    "volume_count_small = \"https://data.cityofnewyork.us/resource/btm5-ppia.csv?$limit=1000000000\"\n",
    "response = requests.get(volume_count_small)\n",
    "\n",
    "# Save the csv file to disk\n",
    "csv_file = \"volume_count_small.csv\"\n",
    "with open(csv_file, \"w\") as f:\n",
    "    f.write(response.text)\n",
    "\n",
    "#Save the csv file to a dataframe    \n",
    "volume_count_small = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the dataframe\n",
    "print(volume_count_small.shape)\n",
    "\n",
    "# Describe the dataframe\n",
    "volume_count_small.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column named all day which is the sum of all columns ending in am or pm\n",
    "volume_count_small[\"value\"] = volume_count_small.filter(regex=\"am|pm\").sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Year column by extracting the year from the date column\n",
    "volume_count_small[\"year\"] = volume_count_small[\"date\"].str[0:4]\n",
    "\n",
    "# Create a Month column by extracting the month from the date column\n",
    "volume_count_small[\"month\"] = volume_count_small[\"date\"].str[5:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe by grouping the volume_count_small dataframe by the Year, Month, roadway_name, direction columns, giving the sum of the all_day column\n",
    "volume_count_small_transformed = volume_count_small.groupby([\"year\", \"month\", \"roadway_name\", \"direction\"]).agg({\"value\": \"sum\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the column names to year, month, location_id, and value\n",
    "volume_count_small_transformed = volume_count_small_transformed.rename(columns={\"roadway_name\": \"location_id\"})\n",
    "\n",
    "# Add a column named 'value_type' with all the values in this column should say 'vehicle_count'\n",
    "volume_count_small_transformed = volume_count_small_transformed.assign(value_type=\"vehicle_count\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap the order of the last 2 columns\n",
    "volume_count_small_transformed = volume_count_small_transformed.iloc[:, [0, 1, 2, 3, 5, 4]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the dataframe\n",
    "print(volume_count_small_transformed.shape)\n",
    "\n",
    "# Describe the dataframe\n",
    "volume_count_small_transformed.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column named 'data_source' with all the values in this column should be 'volume_count_small'\n",
    "volume_count_small_transformed = volume_count_small_transformed.assign(data_source=\"volume_count_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to a csv file\n",
    "volume_count_small_transformed.to_csv(\"volume_count_small_transformed.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Load the csv file\n",
    "volume_count_small_transformed = pd.read_csv('volume_count_small_transformed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "volume_count_small_transformed is one traffic df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Merge all the dataframes into one dataframe\n",
    "traffic = pd.concat([hourly_traffic_transformed, traffic_speeds_transformed, automated_traffic_volume_counts_transformed, daily_traffic_transformed, volume_count_small_transformed], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the dataframe\n",
    "print(traffic.shape)\n",
    "\n",
    "# Describe the dataframe\n",
    "traffic.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Set the year and month columns to integers\n",
    "traffic[\"year\"] = traffic[\"year\"].astype(int)\n",
    "traffic[\"month\"] = traffic[\"month\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum and maximum year\n",
    "print(traffic[\"year\"].min())\n",
    "print(traffic[\"year\"].max())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weather data uploaded from local file\n",
    "\n",
    "Weather data is available from https://scacis.rcc-acis.org/\n",
    "\n",
    "Copied to csv and then loaded locally. All files are available in the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in all the csv files\n",
    "weather_avg_perc = pd.read_csv('NYC Weather - average_percipitation.csv')\n",
    "weather_avg_temp = pd.read_csv('NYC Weather - avg_temp.csv')\n",
    "weather_max_perc = pd.read_csv('NYC Weather - max_precipitation.csv')\n",
    "weather_max_snow_depth = pd.read_csv('NYC Weather - max_snow_depth.csv')\n",
    "weather_max_snowfall = pd.read_csv('NYC Weather - max_snowfall.csv')\n",
    "weather_max_temp = pd.read_csv('NYC Weather - Max_Temp.csv')\n",
    "weather_min_temp = pd.read_csv('NYC Weather - Min_Temp.csv')\n",
    "weather_total_perc = pd.read_csv('NYC Weather - total_percipitation.csv')\n",
    "weather_total_snow_depth = pd.read_csv('NYC Weather - total_snow_depth.csv')\n",
    "weather_total_snowfall = pd.read_csv('NYC Weather - total_snowfall.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the weather dataframes into one dataframe with a column named weather_type to indicate the type of weather\n",
    "weather_avg_perc[\"weather_type\"] = \"average_percipitation\"\n",
    "weather_avg_temp[\"weather_type\"] = \"avg_temp\"\n",
    "weather_max_perc[\"weather_type\"] = \"max_precipitation\"\n",
    "weather_max_snow_depth[\"weather_type\"] = \"max_snow_depth\"\n",
    "weather_max_snowfall[\"weather_type\"] = \"max_snowfall\"\n",
    "weather_max_temp[\"weather_type\"] = \"max_temp\"   \n",
    "weather_min_temp[\"weather_type\"] = \"min_temp\"\n",
    "weather_total_perc[\"weather_type\"] = \"total_percipitation\"\n",
    "weather_total_snow_depth[\"weather_type\"] = \"total_snow_depth\"\n",
    "weather_total_snowfall[\"weather_type\"] = \"total_snowfall\"\n",
    "\n",
    "# Combine all the weather dataframes into one dataframe\n",
    "weather = pd.concat([weather_avg_perc, weather_avg_temp, weather_max_perc, weather_max_snow_depth, weather_max_snowfall, weather_max_temp, weather_min_temp, weather_total_perc, weather_total_snow_depth, weather_total_snowfall], axis=0, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take everything from 2000 onwards\n",
    "weather2000 = weather[weather['Year'] >= 2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather2000.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the Annual column  \n",
    "weather2000 = weather2000.drop(columns=['Annual'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the dataframe so it is in long format\n",
    "weather2001 = weather2000.melt(id_vars=[\"Year\", \"weather_type\"], var_name=\"month\", value_name=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather2001.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the Year column to year\n",
    "weather2001 = weather2001.rename(columns={\"Year\": \"year\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the month column from Jan to 01, Feb to 02, etc.\n",
    "weather2001[\"month\"] = weather2001[\"month\"].str.replace(\"Jan\", \"01\")\n",
    "weather2001[\"month\"] = weather2001[\"month\"].str.replace(\"Feb\", \"02\")\n",
    "weather2001[\"month\"] = weather2001[\"month\"].str.replace(\"Mar\", \"03\")\n",
    "weather2001[\"month\"] = weather2001[\"month\"].str.replace(\"Apr\", \"04\")\n",
    "weather2001[\"month\"] = weather2001[\"month\"].str.replace(\"May\", \"05\")\n",
    "weather2001[\"month\"] = weather2001[\"month\"].str.replace(\"Jun\", \"06\")\n",
    "weather2001[\"month\"] = weather2001[\"month\"].str.replace(\"Jul\", \"07\")\n",
    "weather2001[\"month\"] = weather2001[\"month\"].str.replace(\"Aug\", \"08\")\n",
    "weather2001[\"month\"] = weather2001[\"month\"].str.replace(\"Sep\", \"09\")\n",
    "weather2001[\"month\"] = weather2001[\"month\"].str.replace(\"Oct\", \"10\")\n",
    "weather2001[\"month\"] = weather2001[\"month\"].str.replace(\"Nov\", \"11\")\n",
    "weather2001[\"month\"] = weather2001[\"month\"].str.replace(\"Dec\", \"12\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Swap the order of the last 2 columns\n",
    "weather = weather2001.iloc[:, [0, 2, 1, 3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dataframe to a csv file\n",
    "weather.to_csv(\"weather.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# load the transformed dataframe from a csv file\n",
    "weather = pd.read_csv(\"weather.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random sample of 50 rows\n",
    "weather2002 = weather.sample(n=50, random_state=1) \n",
    "\n",
    "weather2002.describe(include='all')\n",
    "\n",
    "print(weather2002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End of weather data upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bus Delay Time Years are from 2015 - 2024\n",
    "# The Earliest Weather Data is from 2000 - 2024\n",
    "# The Earliest Traffic Data is from 2000 - 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform EDA on the delay data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Change the month column to date and the Year column to year\n",
    "delays = wait_assessment_2015_2020.rename(columns={\"month\": \"date\", \"Year\": \"year\", \"Month\": \"month\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform EDA on the wait_assessment_2015_2020 dataframe\n",
    "delays.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correlation between the columns\n",
    "delays.corr()\n",
    "\n",
    "# Print a correlation heatmap\n",
    "sns.heatmap(delays.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for the columns\n",
    "delays.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the wait Assemsent column and plot a histogram\n",
    "sns.histplot(delays['wait_assessment'], kde=True).set(title='Wait Assessment Distribution', xlabel='Wait Assessment', ylabel='Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the wait Assemsent column and plot a boxplot\n",
    "sns.boxplot(x=delays[\"wait_assessment\"]).set(title='Wait Assessment Boxplot', xlabel='Wait Assessment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if the wait_assessment column is normally distributed\n",
    "stat, p = shapiro(delays['wait_assessment'])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Sample looks Gaussian (fail to reject H0)')\n",
    "else:\n",
    "    print('Sample does not look Gaussian (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the percentiles of the wait_assessment column\n",
    "print(delays[\"wait_assessment\"].quantile([0.25, 0.5, 0.75]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean, median and mode of the wait_assessment column with explanations for their meaning\n",
    "print(f\"The mean of the wait_assessment column is {delays['wait_assessment'].mean()}. \\\n",
    "    The mean is the average of the wait_assessment column. \\\n",
    "    It is the sum of all the values in the wait_assessment column divided by the number of values in the wait_assessment column.\")\n",
    "\n",
    "print(f\"The median of the wait_assessment column is {delays['wait_assessment'].median()}. \\\n",
    "      The median is the middle value of the wait_assessment column. \\\n",
    "      It is the value that splits the wait_assessment column into two halves.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Print the skewness and kurtosis of the wait_assessment column with explanations for their meaning\n",
    "print(f\"The skewness of the wait_assessment column is {delays['wait_assessment'].skew()}. \\\n",
    "    A skewness of 0 means the wait_assessment column is normally distributed. \\\n",
    "    A skewness less than 0 means the wait_assessment column is left skewed. \\\n",
    "    A skewness greater than 0 means the wait_assessment column is right skewed.\")\n",
    "\n",
    "print(f\"The kurtosis of the wait_assessment column is {delays['wait_assessment'].kurt()}. \\\n",
    "    The kurtosis is a measure of the tailedness of the wait_assessment column. \\\n",
    "    A kurtosis of 0 means the wait_assessment column has the same tailedness as a normal distribution. \\\n",
    "    A kurtosis less than 0 means the wait_assessment column is platykurtic. \\\n",
    "    A kurtosis greater than 0 means the wait_assessment column is leptokurtic.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time series plot for the wait assessment data where the route_id is BX1\n",
    "delays[delays[\"route_id\"] == 'BX1'].plot(x=\"date\", y=\"wait_assessment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Create a df with the wait assessment data where the wait_assessment is an average of the wait_assessment column grouped by the year and month columns\n",
    "delays2 = delays.groupby([\"year\", \"month\"]).agg({\"wait_assessment\": \"mean\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display histogram of the wait_assessment column\n",
    "delays2.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time series plot for the wait assessment data\n",
    "delays2.plot(x=\"year\", y=\"wait_assessment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group the dataframe by the date column, giving the mean of the wait_assessment column\n",
    "timeeda = delays.groupby(\"date\").agg({\"wait_assessment\": \"mean\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column to a datetime object\n",
    "timeeda[\"date\"] = pd.to_datetime(timeeda[\"date\"])\n",
    "\n",
    "# Move the date column to the index\n",
    "timeeda = timeeda.set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the Time Series\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(timeeda.index, timeeda[\"wait_assessment\"], color=\"blue\")\n",
    "plt.title(\"Wait Assessment Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Wait Assessment\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeeda.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform EDA on the traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform EDA on the traffic dataframe\n",
    "traffic.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correlation between the columns\n",
    "traffic.corr()\n",
    "\n",
    "# Print a correlation heatmap\n",
    "\n",
    "sns.heatmap(traffic.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create histograms for the columns\n",
    "traffic.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time series plot for the traffic data\n",
    "traffic.plot(x=\"year\", y=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the value counts for the value_type column\n",
    "traffic[\"value_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take only the rows where the value_type is vehicle_count\n",
    "traffic2 = traffic[traffic[\"value_type\"] == \"vehicle_count\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seperate dataframe for the speed data\n",
    "traffic_speed = traffic[traffic[\"value_type\"] == \"speed\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seperate dataframe for the travel_time data\n",
    "traffic_time = traffic[traffic[\"value_type\"] == \"travel_time\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the dataframe\n",
    "traffic_speed.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum and maximum year\n",
    "print(traffic_speed[\"year\"].min())\n",
    "print(traffic_speed[\"year\"].max())\n",
    "\n",
    "print(traffic_time[\"year\"].min())\n",
    "print(traffic_time[\"year\"].max())\n",
    "\n",
    "print(traffic2[\"year\"].min())\n",
    "print(traffic2[\"year\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(traffic2[\"year\"].min())\n",
    "print(traffic2[\"year\"].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the dataframe\n",
    "traffic2.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a seperate dataframe for the traffic data where the location_id is 3\n",
    "traffic3 = traffic2[traffic2[\"location_id\"] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the dataframe\n",
    "traffic3.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take value counts of the data_source column\n",
    "traffic3[\"data_source\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create seperate dataframes for the traffic data where the data_source is hourly_traffic and daily_traffic\n",
    "traffic_hourly = traffic3[traffic3[\"data_source\"] == \"hourly_traffic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sum of the value column grouped by the year and month columns\n",
    "traffic3 = traffic3.groupby([\"year\", \"month\"]).agg({\"value\": \"sum\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a date column by combining the year and month columns\n",
    "traffic3[\"date\"] = traffic3[\"year\"].astype(str) + \"-\" + traffic3[\"month\"].astype(str)\n",
    "\n",
    "# Convert the date column to a date data type\n",
    "traffic3[\"date\"] = pd.to_datetime(traffic3[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time series plot for the traffic data with year and month on the x-axis and value on the y-axis\n",
    "traffic3.plot(x=\"date\", y=\"value\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform EDA on the weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform EDA on the weather dataframe\n",
    "weather.describe(include='all')\n",
    "\n",
    "\n",
    "# Check the correlation between the columns\n",
    "weather.corr()\n",
    "\n",
    "# Print a correlation heatmap\n",
    "sns.heatmap(weather.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Replace any alpha characters with null in the value column\n",
    "weather[\"value\"] = pd.to_numeric(weather[\"value\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the number of null values in the value column grouped by the weather_type column\n",
    "weather.groupby(\"weather_type\").agg({\"value\": \"count\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Create a date column by combining the year and month columns\n",
    "weather[\"date\"] = weather[\"year\"].astype(str) + \"-\" + weather[\"month\"].astype(str)\n",
    "\n",
    "# Convert the date column to a date data type\n",
    "weather[\"date\"] = pd.to_datetime(weather[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time series plot for the weather data with year and month on the x-axis and value on the y-axis where the weather_type is the title\n",
    "weather[weather[\"weather_type\"] == \"max_temp\"].plot(x=\"date\", y=\"value\", title=\"max_temp\")\n",
    "weather[weather[\"weather_type\"] == \"min_temp\"].plot(x=\"date\", y=\"value\", title=\"min_temp\")\n",
    "weather[weather[\"weather_type\"] == \"avg_temp\"].plot(x=\"date\", y=\"value\", title=\"avg_temp\")\n",
    "weather[weather[\"weather_type\"] == \"max_precipitation\"].plot(x=\"date\", y=\"value\", title=\"max_precipitation\")\n",
    "weather[weather[\"weather_type\"] == \"average_percipitation\"].plot(x=\"date\", y=\"value\", title=\"average_percipitation\")\n",
    "weather[weather[\"weather_type\"] == \"total_percipitation\"].plot(x=\"date\", y=\"value\", title=\"total_percipitation\")\n",
    "weather[weather[\"weather_type\"] == \"max_snowfall\"].plot(x=\"date\", y=\"value\", title=\"max_snowfall\")\n",
    "weather[weather[\"weather_type\"] == \"total_snowfall\"].plot(x=\"date\", y=\"value\", title=\"total_snowfall\")\n",
    "weather[weather[\"weather_type\"] == \"max_snow_depth\"].plot(x=\"date\", y=\"value\", title=\"max_snow_depth\")\n",
    "weather[weather[\"weather_type\"] == \"total_snow_depth\"].plot(x=\"date\", y=\"value\", title=\"total_snow_depth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Fill in null values with interpolation\n",
    "weather[\"value\"] = weather.groupby(\"weather_type\")[\"value\"].transform(lambda x: x.interpolate())\n",
    "\n",
    "# Check if there are any remaining null values\n",
    "weather.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the value counts for the weather_type column\n",
    "weather[\"weather_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the date range for avg_temp and max_temp weather types\n",
    "print(weather[weather[\"weather_type\"] == \"avg_temp\"][\"date\"].min())\n",
    "print(weather[weather[\"weather_type\"] == \"avg_temp\"][\"date\"].max())\n",
    "\n",
    "print(weather[weather[\"weather_type\"] == \"max_temp\"][\"date\"].min())\n",
    "print(weather[weather[\"weather_type\"] == \"max_temp\"][\"date\"].max())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an avg_temp dataframe and a max_temp dataframe\n",
    "avg_temp = weather[weather[\"weather_type\"] == \"avg_temp\"]\n",
    "max_temp = weather[weather[\"weather_type\"] == \"max_temp\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the month column to a float data type\n",
    "avg_temp[\"month\"] = avg_temp[\"month\"].astype(float)\n",
    "\n",
    "# describe the avg_temp dataframe\n",
    "avg_temp.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the year and the month values, ordered by year and month\n",
    "avg_temp.sort_values([\"year\", \"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Remove any rows where the date is after 2023-12-01\n",
    "weather = weather[weather[\"date\"] <= \"2023-12-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the weather dataframe\n",
    "weather.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Value Counts on the weather_type column\n",
    "weather[\"weather_type\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistical Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows where period is Systemwide\n",
    "\n",
    "delays_clean = delays[delays[\"period\"] != \"Systemwide\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the rows where borough is Systemwide or Misc\n",
    "delays_clean = delays_clean[delays_clean[\"borough\"] != \"Systemwide\"]\n",
    "delays_clean = delays_clean[delays_clean[\"borough\"] != \"Misc\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_clean.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an anova_delays df grouped by the date, bourough, route id, and period, with the mean of the wait_assessment column\n",
    "anova_delays = delays_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Assumptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group Delays_clean by date, borough and period and get the mean of the wait_assessment column\n",
    "delays_clean = delays_clean.groupby([\"date\", \"borough\", \"period\"]).agg({\"wait_assessment\": \"mean\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print value counts for the period and borough columns\n",
    "delays_clean[\"period\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_clean.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anova_delays.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import levene\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "# Drop rows with NaN values in 'wait_assessment' column\n",
    "anova_delays_cleaned = anova_delays.dropna(subset=['wait_assessment'])\n",
    "\n",
    "# Group the cleaned data\n",
    "grouped_data = anova_delays_cleaned.groupby(['borough', 'period'])['wait_assessment']\n",
    "\n",
    "# Convert the grouped data to a list of arrays\n",
    "data = [group.values for name, group in grouped_data]\n",
    "\n",
    "# Perform Levene's test\n",
    "statistic, p_value = levene(*data)\n",
    "\n",
    "# Check the result\n",
    "if np.isnan(statistic) or np.isnan(p_value):\n",
    "    print(\"Unable to perform Levene's test due to NaN values in the data.\")\n",
    "elif p_value > 0.05:\n",
    "    print(\"Homogeneity of variances is satisfied (p > 0.05)\")\n",
    "else:\n",
    "    print(\"Homogeneity of variances is not satisfied (p <= 0.05)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(statistic, p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_data = anova_delays.groupby(['borough', 'period'])['wait_assessment']\n",
    "\n",
    "for name, group in grouped_data:\n",
    "    # Perform Shapiro-Wilk test for normality\n",
    "    stat, p_value = shapiro(group)\n",
    "    \n",
    "    # Check the result\n",
    "    if p_value > 0.05:\n",
    "        print(f\"Data for {name} is normally distributed (p > 0.05)\")\n",
    "    else:\n",
    "        print(f\"Data for {name} is not normally distributed (p <= 0.05)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table showing the result of the Shapiro-Wilk test for normality for each group\n",
    "shapiro_table = pd.DataFrame(columns=[\"group\", \"p_value\"])\n",
    "\n",
    "for name, group in grouped_data:\n",
    "    stat, p_value = shapiro(group)\n",
    "    shapiro_table = shapiro_table.append({\"group\": name, \"p_value\": p_value}, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the shapiro_table to a csv file\n",
    "shapiro_table.to_csv(\"shapiro_table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create probability plots for the wait_assessment column grouped by the period and borough columns\n",
    "for name, group in grouped_data:\n",
    "    sm.qqplot(group, line ='45')\n",
    "    plt.title(f\"Probability Plot for {name}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the groupings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a boxplot of the wait_assessment column grouped by the borough and period columns\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.boxplot(x=\"borough\", y=\"wait_assessment\", hue=\"period\", data=anova_delays, palette=\"Set1\")\n",
    "plt.xlabel('Borough')\n",
    "plt.ylabel('Wait Assessment')\n",
    "plt.title('Wait Assessment by Borough and Period')\n",
    "plt.legend(title='Period', loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean and standard deviation for each borough and period combination\n",
    "for x in anova_delays['borough'].unique():\n",
    "    for y in anova_delays['period'].unique():\n",
    "        print(x, y, \"Mean :\", round(anova_delays[(anova_delays['borough'] == x) & (anova_delays['period'] == y)]['wait_assessment'].mean(),2))\n",
    "        print(x, y, \"Standard Deviation :\", round(anova_delays[(anova_delays['borough'] == x) & (anova_delays['period'] == y)]['wait_assessment'].std(),2),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with the mean and standard deviation for each borough and period combination and save it to a csv file\n",
    "anova_delays.groupby(['borough', 'period'])['wait_assessment'].agg(['mean', 'std']).to_csv(\"delays_mean_stddev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the data types of the columns\n",
    "print(anova_delays.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform borough and period to categorical data types\n",
    "anova_delays[\"borough\"] = anova_delays[\"borough\"].astype(\"category\")\n",
    "anova_delays[\"period\"] = anova_delays[\"period\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform an ANOVA on the delay data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform two-way ANOVA\n",
    "# The effect of borough, the effect of period, and the interaction between borough and period\n",
    "model = ols('wait_assessment ~ borough + period + borough:period', data = anova_delays).fit()\n",
    "aov2 = sm.stats.anova_lm(model, type=2)\n",
    "print(aov2, '\\n')\n",
    "\n",
    "# print the results of the two-way ANOVA hypothesis tests\n",
    "print(f\"Borough p-value: {aov2['PR(>F)'][0]:.4f}\")\n",
    "print(f\"Period p-value: {aov2['PR(>F)'][1]:.4f}\")\n",
    "print(f\"Interaction p-value: {aov2['PR(>F)'][2]:.4f}\", '\\n')\n",
    "\n",
    "# Print the results of the two-way ANOVA hypothesis tests\n",
    "if aov2['PR(>F)'][0] < 0.05:\n",
    "    print(\"Reject null hypothesis - Significant differences exist between Borough groups.\")\n",
    "else:\n",
    "    print(\"Accept null hypothesis - No significant difference between Borough groups.\")\n",
    "    \n",
    "if aov2['PR(>F)'][1] < 0.05:\n",
    "    print(\"Reject null hypothesis - Significant differences exist between Period groups.\")\n",
    "else:\n",
    "    print(\"Accept null hypothesis - No significant difference between Period groups.\")\n",
    "    \n",
    "if aov2['PR(>F)'][2] < 0.05:\n",
    "    print(\"Reject null hypothesis - Interaction occurs between factors.\")\n",
    "else:\n",
    "    print(\"Accept null hypothesis - No interaction occurs between factors.\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store to aov2 in a table and save it to a csv file\n",
    "aov2.to_csv(\"aov2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'period' and 'borough' columns to string type\n",
    "anova_delays['period'] = anova_delays['period'].astype(str)\n",
    "anova_delays['borough'] = anova_delays['borough'].astype(str)\n",
    "\n",
    "# Plot the interaction plot\n",
    "fig = interaction_plot(x=anova_delays['period'].values, trace=anova_delays['borough'].values, response=anova_delays['wait_assessment'].values)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Post Hoc Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a Tukey's Range Test to determine which groups are significantly different from each other\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Perform Tukey's Range Test\n",
    "tukey_results = pairwise_tukeyhsd(anova_delays['wait_assessment'], anova_delays['borough'], 0.05)\n",
    "\n",
    "# Print the results\n",
    "print(tukey_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a Tukey's Range Test to determine which groups are significantly different from each other\n",
    "from statsmodels.stats.multicomp import pairwise_tukeyhsd\n",
    "\n",
    "# Perform Tukey's Range Test\n",
    "tukey_results = pairwise_tukeyhsd(anova_delays['wait_assessment'], anova_delays['period'], 0.05)\n",
    "\n",
    "# Print the results\n",
    "print(tukey_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Kruskal-Wallus Test for Periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the date distribution of the wait_assessment column\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.boxplot(x=\"period\", y=\"wait_assessment\", data=delays_clean, palette=\"Set1\")\n",
    "plt.xlabel('Period')\n",
    "plt.ylabel('Wait Assessment')\n",
    "plt.title('Wait Assessment by Period')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Kruskal-Wallis test\n",
    "stat, p = kruskal(*[delays_clean[delays_clean['period'] == rate]['wait_assessment'] for rate in delays_clean['period'].unique()])\n",
    "\n",
    "# Print the results\n",
    "print(\"Kruskal-Wallis Test Results:\")\n",
    "print(f\"Test statistic: {stat:.4f}\")\n",
    "print(f\"P-value: {p:.4f}\")\n",
    "\n",
    "# Calculate the critical value for the significance level of 0.05\n",
    "critical_value = chi2.ppf(1-0.05, 2)\n",
    "\n",
    "print(f\"Critical value: {critical_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform post-hoc test\n",
    "sp.posthoc_dunn([delays_clean[delays_clean['period'] == rate]['wait_assessment'] for rate in delays_clean['period'].unique()], p_adjust='holm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Kruska-Wallis Test for Boroughs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise the date distribution of the wait_assessment column\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.boxplot(x=\"borough\", y=\"wait_assessment\", data=delays_clean, palette=\"Set1\")\n",
    "plt.xlabel('Borough')\n",
    "plt.ylabel('Wait Assessment')\n",
    "plt.title('Wait Assessment by Borough')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the median value of the wait_assessment column grouped by the borough\n",
    "print(delays_clean.groupby(\"borough\")[\"wait_assessment\"].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform Kruskal-Wallis test\n",
    "stat, p = kruskal(*[delays_clean[delays_clean['borough'] == rate]['wait_assessment'] for rate in delays_clean['borough'].unique()])\n",
    "\n",
    "# Print the results\n",
    "print(\"Kruskal-Wallis Test Results:\")\n",
    "print(f\"Test statistic: {stat:.4f}\")\n",
    "print(f\"P-value: {p:.4f}\")\n",
    "\n",
    "# Calculate the critical value for the significance level of 0.05\n",
    "critical_value = chi2.ppf(1-0.05, 2)\n",
    "\n",
    "print(f\"Critical value: {critical_value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform post-hoc test\n",
    "sp.posthoc_dunn([delays_clean[delays_clean['borough'] == rate]['wait_assessment'] for rate in delays_clean['borough'].unique()], p_adjust='holm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the post-hoc test results to a table and export it to a csv file\n",
    "sp.posthoc_dunn([delays_clean[delays_clean['borough'] == rate]['wait_assessment'] for rate in delays_clean['borough'].unique()], p_adjust='holm').to_csv(\"posthoc_dunn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which Weather Data to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Weather Data in wide format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Melt the weather dataframe so all the weather types are in seperate columns, with only the date column remaining\n",
    "weather_wide = weather.pivot(index=\"date\", columns=\"weather_type\", values=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the weather_wide dataframe\n",
    "weather_wide.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the delays dataframe\n",
    "delays2.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# create a date column by combining the year and month columns\n",
    "delays2[\"date\"] = delays2[\"year\"].astype(str) + \"-\" + delays2[\"month\"].astype(str)\n",
    "\n",
    "# Convert the date column to a date data type\n",
    "delays2[\"date\"] = pd.to_datetime(delays2[\"date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join with Delay Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "delays_weather = pd.merge(weather_wide, delays2, on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the delays_weather dataframe\n",
    "delays_weather.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the Null values in the delays_weather dataframe\n",
    "delays_weather.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Remove the Null values from the delays_weather dataframe\n",
    "delays_weather = delays_weather.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform PCA to understand which weather data has the most impact on delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays_weather.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Separate the features (weather type columns) from the target (wait assessment column)\n",
    "features = delays_weather.drop(columns=['date', 'wait_assessment', 'year', 'month'])\n",
    "target = delays_weather['wait_assessment']\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(features)\n",
    "\n",
    "# Get the explained variance ratio for each principal component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Create a dataframe to store the results\n",
    "pca_results = pd.DataFrame({'Weather Type': features.columns, 'Explained Variance Ratio': explained_variance_ratio})\n",
    "\n",
    "# Sort the dataframe by the explained variance ratio in descending order, round the values to 2 decimal places\n",
    "pca_results = pca_results.round(2)\n",
    "pca_results = pca_results.sort_values(by='Explained Variance Ratio', ascending=False)\n",
    "\n",
    "# Print the results\n",
    "print(pca_results)\n",
    "\n",
    "# Print the Top 3 Weather Types\n",
    "print(pca_results.iloc[0:3, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Keep only the avg_perc and avg_temp columns\n",
    "weather_features = delays_weather[['date', 'average_percipitation', 'avg_temp']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Average Percipitation and Average Temperature as the weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weathereda = weather_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column to a datetime object\n",
    "weathereda[\"date\"] = pd.to_datetime(weathereda[\"date\"])\n",
    "\n",
    "# Move the date column to the index\n",
    "weathereda = weathereda.set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the average_percipitation column\n",
    "sns.histplot(weather_features['average_percipitation'], kde=True).set(title='Average Percipitation Distribution', xlabel='Average Percipitation', ylabel='Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the box plot\n",
    "plt.boxplot(weather_features[\"average_percipitation\"], labels=[\"Average Percipitation\"])\n",
    "\n",
    "# Add labels for min, max, and median values\n",
    "min_val = weather_features[\"average_percipitation\"].min()\n",
    "max_val = weather_features[\"average_percipitation\"].max()\n",
    "median_val = weather_features[\"average_percipitation\"].median()\n",
    "\n",
    "plt.text(1, min_val, f\"Min: {min_val:.2f}\", ha='center', va='bottom')\n",
    "plt.text(1, max_val, f\"Max: {max_val:.2f}\", ha='center', va='top')\n",
    "plt.text(1, median_val, f\"Median: {median_val:.2f}\", ha='center', va='bottom')\n",
    "\n",
    "# Set the title and x-axis label\n",
    "plt.title('Average Percipitation Boxplot')\n",
    "plt.xlabel('Average Percipitation')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean and median of the average_percipitation column\n",
    "print(f\"The mean of the average_percipitation column is {weather_features['average_percipitation'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the 25th, 50th, 75th and 95th percentiles of the average_percipitation column\n",
    "print(weather_features[\"average_percipitation\"].quantile([0.25, 0.5, 0.75, 0.95]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The median of the average_percipitation column is {weather_features['average_percipitation'].median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the skewness and kurtosis of the average_percipitation column\n",
    "print(f\"The skewness of the average_percipitation column is {weather_features['average_percipitation'].skew()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the avg_temp column\n",
    "sns.histplot(weather_features['avg_temp'], kde=True).set(title='Average Temperature Distribution', xlabel='Average Temperature', ylabel='Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean and median of the average tempeture column\n",
    "print(f\"The mean of the avg_temp column is {weather_features['avg_temp'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The median of the avg_temp column is {weather_features['avg_temp'].median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the skewness and kurtosis of the average tempeture column\n",
    "print(f\"The skewness of the avg_temp column is {weather_features['avg_temp'].skew()}\")\n",
    "print(f\"The kurtosis of the avg_temp column is {weather_features['avg_temp'].kurt()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a time series plot for the average tempeture column\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(weathereda.index, weathereda[\"avg_temp\"], color=\"blue\")\n",
    "plt.title(\"Average Temperature Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Average Temperature\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Which Traffic Data to Use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get Traffic Data in wide format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the traffic dataframe\n",
    "traffic.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "traffic_wide = traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate the 'value_type' and 'data_source' columns to create a new column named 'traffic_type'\n",
    "traffic_wide[\"traffic_type\"] = traffic_wide[\"value_type\"] + \"_\" + traffic_wide[\"data_source\"]\n",
    "\n",
    "# Get rid of the 'value_type' and 'data_source' columns\n",
    "traffic_wide = traffic_wide.drop(columns=[\"value_type\", \"data_source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concate the 'location_id' and 'traffic_type' columns to create a new column named 'location_traffic_type'\n",
    "#traffic_wide[\"location_traffic_type\"] = traffic_wide[\"location_id\"].astype(str) + \"_\" + traffic_wide[\"traffic_type\"]\n",
    "\n",
    "# Get rid of the 'location_id' column\n",
    "#traffic_wide = traffic_wide.drop(columns=[\"location_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Create a date column by combining the year and month columns\n",
    "traffic_wide[\"date\"] = traffic_wide[\"year\"].astype(str) + \"-\" + traffic_wide[\"month\"].astype(str)\n",
    "\n",
    "# Convert the date column to a date data type\n",
    "traffic_wide[\"date\"] = pd.to_datetime(traffic_wide[\"date\"])\n",
    "\n",
    "# Drop the year and month columns\n",
    "traffic_wide = traffic_wide.drop(columns=[\"year\", \"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the traffic_wide dataframe\n",
    "traffic_wide.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Transform the traffic_wide dataframe so that the values for where the location_id and traffic_type columns are added together\n",
    "traffic_wide = traffic_wide.groupby([\"date\", \"location_id\", \"traffic_type\"]).agg({\"value\": \"sum\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Transform the traffic_wide dataframe so that the values for where the location_id and traffic_type are the same are averaged together\n",
    "traffic_wide = traffic_wide.groupby([\"date\", \"traffic_type\"]).agg({\"value\": \"mean\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Melt the traffic_wide dataframe so all the traffic_type column values are in seperate columns, with only the date column remaining\n",
    "traffic_wide = traffic_wide.pivot(index=\"date\", columns=\"traffic_type\", values=\"value\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the traffic_wide dataframe\n",
    "traffic_wide.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join with Delay Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Join the traffic_wide and delays dataframes together\n",
    "traffic_delays = pd.merge(traffic_wide, delays2, on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the traffic_delays dataframe\n",
    "traffic_delays.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Null values in the traffic_delays dataframe\n",
    "traffic_delays.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Take only the rows where the wait_assessment column is not null\n",
    "traffic_delays = traffic_delays[traffic_delays[\"wait_assessment\"].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a time series graph of the traffic columns in the traffic_delays dataframe\n",
    "traffic_delays.plot(x=\"date\", y=[\"speed_traffic_speeds\", \"travel_time_traffic_speeds\",\"vehicle_count_automated_traffic_volume_counts\",\"vehicle_count_daily_traffic\", \"vehicle_count_hourly_traffic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Use interpolation to fill in the Null values in the traffic_delays dataframe\n",
    "traffic_delays = traffic_delays.interpolate(method='pad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Remove the vehicle_count_volume_count_small column and all rows after 2023-12-01\n",
    "traffic_delays = traffic_delays.drop(columns=[\"vehicle_count_volume_count_small\", \"travel_time_traffic_speeds\", \"speed_traffic_speeds\", \"vehicle_count_daily_traffic\"])\n",
    "traffic_delays = traffic_delays[traffic_delays[\"date\"] <= \"2023-12-01\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_delays.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of Null values in the traffic_delays dataframe\n",
    "traffic_delays.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform PCA to understand which traffic data has the most impact on delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features (traffic type columns) from the target (wait assessment column)\n",
    "features = traffic_delays.drop(columns=['date', 'wait_assessment', 'year', 'month'])\n",
    "target = traffic_delays['wait_assessment']\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "pca.fit(features)\n",
    "\n",
    "# Get the explained variance ratio for each principal component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Create a dataframe to store the results\n",
    "pca_results = pd.DataFrame({'Traffic Type': features.columns, 'Explained Variance Ratio': explained_variance_ratio})\n",
    "\n",
    "# Sort the dataframe by the explained variance ratio in descending order, round the values to 2 decimal places\n",
    "pca_results = pca_results.round(2)\n",
    "pca_results = pca_results.sort_values(by='Explained Variance Ratio', ascending=False)\n",
    "\n",
    "# Print the results\n",
    "print(pca_results)\n",
    "\n",
    "# Print the Top 2 Traffic Types\n",
    "print(pca_results.iloc[0:2, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Keep only the avg_perc and avg_temp columns\n",
    "traffic_features = traffic_delays[['date', 'vehicle_count_automated_traffic_volume_counts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trafficeda = traffic_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column to a datetime object\n",
    "trafficeda[\"date\"] = pd.to_datetime(trafficeda[\"date\"])\n",
    "\n",
    "# Move the date column to the index\n",
    "trafficeda = trafficeda.set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the normal distribution of the vehicle_count_automated_traffic_volume_counts column\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.histplot(trafficeda['vehicle_count_automated_traffic_volume_counts'], kde=True).set(title='Vehicle Count Automated Traffic Volume Counts Distribution', xlabel='Vehicle Count Automated Traffic Volume Counts', ylabel='Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the mean and median values of the vehicle_count_automated_traffic_volume_counts column\n",
    "print(f\"The mean of the vehicle_count_automated_traffic_volume_counts column is {trafficeda['vehicle_count_automated_traffic_volume_counts'].mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The median of the vehicle_count_automated_traffic_volume_counts column is {trafficeda['vehicle_count_automated_traffic_volume_counts'].median()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the skewness and kurtosis of the vehicle_count_automated_traffic_volume_counts column\n",
    "print(f\"The skewness of the vehicle_count_automated_traffic_volume_counts column is {trafficeda['vehicle_count_automated_traffic_volume_counts'].skew()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The kurtosis of the vehicle_count_automated_traffic_volume_counts column is {trafficeda['vehicle_count_automated_traffic_volume_counts'].kurt()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the normality of the vehicle_count_automated_traffic_volume_counts column\n",
    "stat, p = shapiro(trafficeda['vehicle_count_automated_traffic_volume_counts'])\n",
    "print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "\n",
    "# interpret\n",
    "alpha = 0.05\n",
    "if p > alpha:\n",
    "    print('Sample looks Gaussian (fail to reject H0)')\n",
    "else:\n",
    "    print('Sample does not look Gaussian (reject H0)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a time series plot for the vehicle_count_automated_traffic_volume_counts data\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(trafficeda.index, trafficeda[\"vehicle_count_automated_traffic_volume_counts\"], color=\"red\")\n",
    "plt.title(\"Vehicle Count Automated Traffic Volume Counts Over Time\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Vehicle Count Automated Traffic Volume Counts\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use vehicle_count_automated_traffic_volume_counts as the traffic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform a Time Series Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the data for the model, combining the delays, weather and traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Drop the year and month columns\n",
    "delays2 = delays2.drop(columns=[\"year\", \"month\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delays2.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_features.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traffic_features.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Join the weather_features and traffic_features and delays2 dataframes together\n",
    "features = pd.merge(delays2, weather_features, on=\"date\", how=\"left\")\n",
    "features = pd.merge(features, traffic_features, on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the features dataframe\n",
    "features.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decompose the time series to understand the trend and seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the date column to a date data type\n",
    "features[\"date\"] = pd.to_datetime(features[\"date\"])\n",
    "\n",
    "# Have the date column as the index\n",
    "features = features.set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# set the frequncey of the index to monthly\n",
    "features.index = pd.date_range(start='2015-01-01', periods=len(features), freq='M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose_data = seasonal_decompose(features['wait_assessment'], model=\"additive\")\n",
    "decompose_data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decompose_data = seasonal_decompose(features['wait_assessment'], model=\"multiplicative\")\n",
    "\n",
    "decompose_data.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seasonality=decompose_data.seasonal\n",
    "seasonality.plot(color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the stationarity of the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = adfuller(features.wait_assessment, autolag = 'AIC')\n",
    "print(\"1. ADF : \",dftest[0])\n",
    "print(\"2. P-Value : \", dftest[1])\n",
    "print(\"3. Num Of Lags : \", dftest[2])\n",
    "print(\"4. Num Of Observations Used For ADF Regression and Critical Values Calculation :\", dftest[3])\n",
    "print(\"5. Critical Values :\")\n",
    "for key, val in dftest[4].items():\n",
    "    print(\"\\t\",key, \": \", val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Use a rolling mean to smooth out the data and make it stationary\n",
    "rolling_mean = features['wait_assessment'].rolling(window = 12).mean()\n",
    "\n",
    "# Create a column named rolling_mean in the features dataframe which is the rolling mean of the wait_assessment column with a window of 12 months minus the rolling mean of the wait_assessment column with a window of 12 months shifted by 1 month\n",
    "features['rolling_mean_diff'] = rolling_mean - rolling_mean.shift()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['wait_assessment'].plot(title='Original')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features['rolling_mean_diff'].plot(title='Post Rolling Mean & Differencing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the features dataframe\n",
    "features.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "#Check for Null values in the features dataframe\n",
    "features.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Use interpolation to fill in the Null values in the features dataframe\n",
    "features = features.interpolate(method='pad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Show the index for the rows of rolling_mean_diff where the value is null\n",
    "features[features['rolling_mean_diff'].isnull()].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Fill in the Null values in the rolling_mean_diff column With the mean of the rolling_mean_diff column\n",
    "features['rolling_mean_diff'] = features['rolling_mean_diff'].fillna(features['rolling_mean_diff'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest = adfuller(features.rolling_mean_diff, autolag = 'AIC')\n",
    "print(\"1. ADF : \",dftest[0])\n",
    "print(\"2. P-Value : \", dftest[1])\n",
    "print(\"3. Num Of Lags : \", dftest[2])\n",
    "print(\"4. Num Of Observations Used For ADF Regression and Critical Values Calculation :\", dftest[3])\n",
    "print(\"5. Critical Values :\")\n",
    "for key, val in dftest[4].items():\n",
    "    print(\"\\t\",key, \": \", val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a Sarimax model to predict the delays\n",
    "\n",
    "Use the weather and traffic data as exogenous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the endogenous variable (target)\n",
    "endog = features['rolling_mean_diff']\n",
    "\n",
    "# Define the exogenous variables\n",
    "exog = features[['average_percipitation', 'avg_temp', 'vehicle_count_automated_traffic_volume_counts']]\n",
    "\n",
    "# Add a constant to the exogenous variables\n",
    "exog = sm.add_constant(exog)\n",
    "\n",
    "# Perform test/train split\n",
    "train_size = 0.8  \n",
    "endog_train, endog_test, exog_train, exog_test = train_test_split(\n",
    "    endog, exog, train_size=train_size, shuffle=False\n",
    ")\n",
    "\n",
    "# Set the frequency to monthly\n",
    "freq = \"M\"\n",
    "\n",
    "# Create the SARIMAX model for training\n",
    "model = sm.tsa.SARIMAX(endog_train, exog=exog_train, order=(1, 0, 0), seasonal_order=(0, 0, 0, 12), freq=freq)\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(results.summary())\n",
    "\n",
    "# Predict on the test set\n",
    "forecast = results.get_forecast(steps=len(endog_test), exog=exog_test)\n",
    "predicted_values = forecast.predicted_mean\n",
    "\n",
    "# Evaluate the accuracy (you can use any appropriate metric depending on your task)\n",
    "mae = mean_absolute_error(endog_test, predicted_values)\n",
    "rmse = np.sqrt(mean_squared_error(endog_test, predicted_values))\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": ""
    }
   },
   "outputs": [],
   "source": [
    "# Scale the numerical features in the features dataframe using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features[['average_percipitation', 'avg_temp', 'vehicle_count_automated_traffic_volume_counts', 'wait_assessment', 'rolling_mean_diff']] = scaler.fit_transform(features[['average_percipitation', 'avg_temp', 'vehicle_count_automated_traffic_volume_counts', 'wait_assessment', 'rolling_mean_diff']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the endogenous variable (target)\n",
    "endog = features['rolling_mean_diff']\n",
    "\n",
    "# Define the exogenous variables\n",
    "exog = features[['average_percipitation', 'avg_temp', 'vehicle_count_automated_traffic_volume_counts']]\n",
    "\n",
    "# Add a constant to the exogenous variables\n",
    "exog = sm.add_constant(exog)\n",
    "\n",
    "# Perform test/train split\n",
    "train_size = 0.8  # You can adjust this ratio as needed\n",
    "endog_train, endog_test, exog_train, exog_test = train_test_split(\n",
    "    endog, exog, train_size=train_size, shuffle=False\n",
    ")\n",
    "\n",
    "# Set the frequency to monthly\n",
    "freq = \"M\"\n",
    "\n",
    "# Create the SARIMAX model for training\n",
    "model = sm.tsa.SARIMAX(endog_train, exog=exog_train, order=(1, 0, 0), seasonal_order=(0, 0, 0, 0), freq=freq)\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(results.summary())\n",
    "\n",
    "# Predict on the test set\n",
    "forecast = results.get_forecast(steps=len(endog_test), exog=exog_test)\n",
    "predicted_values = forecast.predicted_mean\n",
    "\n",
    "# Evaluate the accuracy (you can use any appropriate metric depending on your task)\n",
    "mae = mean_absolute_error(endog_test, predicted_values)\n",
    "rmse = np.sqrt(mean_squared_error(endog_test, predicted_values))\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the endogenous variable (target)\n",
    "endog = features['rolling_mean_diff']\n",
    "\n",
    "# Define the exogenous variables\n",
    "exog = features[['average_percipitation', 'avg_temp', 'vehicle_count_automated_traffic_volume_counts']]\n",
    "\n",
    "# Add a constant to the exogenous variables\n",
    "exog = sm.add_constant(exog)\n",
    "\n",
    "# Perform test/train split\n",
    "train_size = 0.8\n",
    "endog_train, endog_test, exog_train, exog_test = train_test_split(\n",
    "    endog, exog, train_size=train_size, shuffle=False\n",
    ")\n",
    "\n",
    "# Define the range of parameters to search\n",
    "p_values = range(0, 3)  \n",
    "d_values = range(0, 2)  \n",
    "q_values = range(0, 3)  \n",
    "P_values = range(0, 2)  \n",
    "D_values = range(0, 2) \n",
    "Q_values = range(0, 2)\n",
    "\n",
    "# Create a list of all possible parameter combinations\n",
    "param_combinations = list(itertools.product(p_values, d_values, q_values, P_values, D_values, Q_values))\n",
    "\n",
    "best_mae = float('inf')\n",
    "best_params = None\n",
    "\n",
    "# Perform grid search\n",
    "for params in param_combinations:\n",
    "    order = (params[0], params[1], params[2])\n",
    "    seasonal_order = (params[3], params[4], params[5], 12)\n",
    "    \n",
    "    # Specify the frequency of the time series\n",
    "    freq = 'M'\n",
    "    \n",
    "    # Create a datetime index with the specified frequency\n",
    "    date_index = pd.date_range(start=features.index.min(), end=features.index.max(), freq=freq)\n",
    "    \n",
    "    # Reindex the endogenous and exogenous variables\n",
    "    #endog = endog.reindex(date_index)\n",
    "    exog = exog.reindex(date_index)\n",
    "\n",
    "    # Create the SARIMAX model for training\n",
    "    model = sm.tsa.SARIMAX(endog_train, exog=exog_train, order=order, seasonal_order=seasonal_order, freq=freq)\n",
    "\n",
    "    # Fit the model\n",
    "    results = model.fit(maxiter=5000, disp=True)\n",
    "\n",
    "    # Predict on the test set\n",
    "    forecast = results.get_forecast(steps=len(endog_test), exog=exog_test)\n",
    "    predicted_values = forecast.predicted_mean\n",
    "\n",
    "    # Evaluate the accuracy using Mean Absolute Error (MAE)\n",
    "    mae = mean_absolute_error(endog_test, predicted_values)\n",
    "\n",
    "    # Check if the current combination has a lower MAE\n",
    "    if mae < best_mae:\n",
    "        best_mae = mae\n",
    "        best_params = params\n",
    "\n",
    "# Print the best parameters and corresponding MAE\n",
    "print(f\"Best Parameters: {best_params}\")\n",
    "print(f\"Best Mean Absolute Error: {best_mae:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the endogenous variable (target)\n",
    "endog = features['rolling_mean_diff']\n",
    "\n",
    "# Define the exogenous variables\n",
    "exog = features[['average_percipitation', 'avg_temp', 'vehicle_count_automated_traffic_volume_counts']]\n",
    "\n",
    "# Add a constant to the exogenous variables\n",
    "exog = sm.add_constant(exog)\n",
    "\n",
    "# Perform test/train split\n",
    "train_size = 0.8 \n",
    "endog_train, endog_test, exog_train, exog_test = train_test_split(\n",
    "    endog, exog, train_size=train_size, shuffle=False\n",
    ")\n",
    "\n",
    "# Set the frequency of the time series to monthly\n",
    "freq = 'M'\n",
    "\n",
    "# Create the SARIMAX model for training\n",
    "model = sm.tsa.SARIMAX(endog_train, exog=exog_train, order=(1, 0, 0), seasonal_order=(0, 0, 1, 12), freq=freq)\n",
    "\n",
    "# Fit the model\n",
    "results = model.fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(results.summary())\n",
    "\n",
    "# Predict on the test set\n",
    "forecast = results.get_forecast(steps=len(endog_test), exog=exog_test)\n",
    "predicted_values = forecast.predicted_mean\n",
    "\n",
    "# Evaluate the accuracy (you can use any appropriate metric depending on your task)\n",
    "mae = mean_absolute_error(endog_test, predicted_values)\n",
    "rmse = np.sqrt(mean_squared_error(endog_test, predicted_values))\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the model on the entire dataset to be used in the hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a full model for training the hybrid model\n",
    "\n",
    "# Define the endogenous variable (target)\n",
    "endog1 = features['wait_assessment']\n",
    "\n",
    "# Define the exogenous variables\n",
    "exog1 = features[['average_percipitation', 'avg_temp', 'vehicle_count_automated_traffic_volume_counts']]\n",
    "\n",
    "# Add a constant to the exogenous variables\n",
    "exog1 = sm.add_constant(exog1)\n",
    "\n",
    "# Set the frequency of the time series to monthly\n",
    "freq = 'M'\n",
    "\n",
    "# Create the SARIMAX model for training\n",
    "model1 = sm.tsa.SARIMAX(endog1, exog=exog1, order=(0, 1, 0), seasonal_order=(0, 1, 1, 12), freq=freq)\n",
    "\n",
    "# Fit the model\n",
    "results1 = model1.fit()\n",
    "\n",
    "# Print the summary of the model\n",
    "print(results1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the endog_train, endog_test, exog_train, exog_test dataframe shapes\n",
    "print(endog_train.shape)\n",
    "print(endog_test.shape)\n",
    "print(exog_train.shape)\n",
    "print(exog_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the Actual vs Predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual values from the training set\n",
    "plt.plot(endog_train.index, endog_train, label='Actual (Training Set)', color='blue')\n",
    "\n",
    "# Plot the actual values from the test set\n",
    "plt.plot(endog_test.index, endog_test, label='Actual (Test Set)', color='blue', linestyle='--')\n",
    "\n",
    "# Plot the predicted values from the test set\n",
    "plt.plot(endog_test.index, predicted_values, label='Predicted (Test Set)', color='orange')\n",
    "\n",
    "# Set the labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Wait Assessment')\n",
    "plt.title('Actual vs Predicted Values')\n",
    "\n",
    "# Show the legend\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the Data for the Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the data for the model, combining the delays, weather and traffic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Join the delays2 and features dataframes together\n",
    "features2 = pd.merge(delays2, traffic_features, on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "features2 = pd.merge(features2, weather_features, on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features2.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment the data with a categorical variable showing the traffic and weather performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vehicle_count_automated_traffic_volume_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the wait_assessment column\n",
    "features2['vehicle_count_automated_traffic_volume_counts'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Percentile Thresholds\n",
    "from numpy import array\n",
    "rating_pctile = np.percentile(features2['vehicle_count_automated_traffic_volume_counts'], [75, 90])\n",
    "# The percentile thresholds are\n",
    "rating_pctile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column named w_rating in the features2 dataframe \n",
    "features2['vc_rating'] = 0\n",
    "features2['vc_rating'] = np.where(features2['vehicle_count_automated_traffic_volume_counts'] < rating_pctile[0], 1, features2['vc_rating'])\n",
    "features2['vc_rating'] = np.where((features2['vehicle_count_automated_traffic_volume_counts'] >= rating_pctile[0]) & (features2['vehicle_count_automated_traffic_volume_counts'] <= rating_pctile[1]), 2, features2['vc_rating'])\n",
    "features2['vc_rating'] = np.where(features2['vehicle_count_automated_traffic_volume_counts'] > rating_pctile[1], 3, features2['vc_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a histogram of the w_rating column\n",
    "features2['vc_rating'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "average_percipitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the wait_assessment column\n",
    "features2['average_percipitation'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Percentile Thresholds\n",
    "from numpy import array\n",
    "rating_pctile = np.percentile(features2['average_percipitation'], [75, 90])\n",
    "# The percentile thresholds are\n",
    "rating_pctile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column named w_rating in the features2 dataframe \n",
    "features2['ap_rating'] = 0\n",
    "features2['ap_rating'] = np.where(features2['average_percipitation'] < rating_pctile[0], 1, features2['ap_rating'])\n",
    "features2['ap_rating'] = np.where((features2['average_percipitation'] >= rating_pctile[0]) & (features2['average_percipitation'] <= rating_pctile[1]), 2, features2['ap_rating'])\n",
    "features2['ap_rating'] = np.where(features2['average_percipitation'] > rating_pctile[1], 3, features2['ap_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a histogram of the w_rating column\n",
    "features2['ap_rating'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "avg_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of the wait_assessment column\n",
    "features2['avg_temp'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Percentile Thresholds\n",
    "from numpy import array\n",
    "rating_pctile = np.percentile(features2['avg_temp'], [75, 90])\n",
    "# The percentile thresholds are\n",
    "rating_pctile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column named w_rating in the features2 dataframe \n",
    "features2['at_rating'] = 0\n",
    "features2['at_rating'] = np.where(features2['avg_temp'] < rating_pctile[0], 1, features2['at_rating'])\n",
    "features2['at_rating'] = np.where((features2['avg_temp'] >= rating_pctile[0]) & (features2['avg_temp'] <= rating_pctile[1]), 2, features2['at_rating'])\n",
    "features2['at_rating'] = np.where(features2['avg_temp'] > rating_pctile[1], 3, features2['at_rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a histogram of the w_rating column\n",
    "features2['at_rating'].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment the data with time based variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function for the feature engineering, creating time based features\n",
    "def add_datepart(df, fldname, drop=True):\n",
    "    fld = df[fldname]\n",
    "    if not np.issubdtype(fld.dtype, np.datetime64):\n",
    "        df[fldname] = fld = pd.to_datetime(fld, \n",
    "                                     infer_datetime_format=True)\n",
    "    targ_pre = re.sub('[Dd]ate$', '', fldname)\n",
    "    for n in ('Year', 'Month', 'Week', 'Day', 'Dayofweek', \n",
    "            'Dayofyear', 'Is_month_end', 'Is_month_start', \n",
    "            'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', \n",
    "            'Is_year_start'):\n",
    "        df[targ_pre+n] = getattr(fld.dt,n.lower())        \n",
    "    df[targ_pre+'Elapsed'] = fld.astype(np.int64) // 10**9\n",
    "    if drop: df.drop(fldname, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column to a date data type\n",
    "features2[\"date\"] = pd.to_datetime(features2[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_datepart(features2, 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns that are not needed: Week, Day, Dayofweek, Dayofyear, Is_month_end, Is_month_start, Is_quarter_end, Is_year_end\n",
    "features2 = features2.drop(columns=[\"Week\", \"Day\", \"Dayofweek\", \"Dayofyear\", \"Is_month_end\", \"Is_month_start\", \"Is_quarter_end\", \"Is_year_end\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the is quarter start and is year start columns to int\n",
    "features2[\"Is_quarter_start\"] = features2[\"Is_quarter_start\"].astype(int)\n",
    "features2[\"Is_year_start\"] = features2[\"Is_year_start\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features2.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a sliding window to shift the data and create samples for a supervised learning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features3 = features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop all columns except for wait_assessment\n",
    "features4 = features3.drop(columns=[\"average_percipitation\", \"avg_temp\", \"vehicle_count_automated_traffic_volume_counts\", \"vc_rating\", \"ap_rating\", \"at_rating\", \"Year\", \"Month\", \"Elapsed\", \"Is_quarter_start\", \"Is_year_start\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features4.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the values from features4 into an array\n",
    "features_array = features4.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing a function to transform the time series dataset into a supervised learning dataset\n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "   n_vars = 1 if isinstance(data, list) else data.shape[1]\n",
    "   df = DataFrame(data)\n",
    "   cols = list()\n",
    "   for i in range(n_in, 0, -1):\n",
    "       cols.append(df.shift(i))\n",
    "   for i in range(0, n_out):\n",
    "       cols.append(df.shift(-i))\n",
    "   agg = concat(cols, axis=1)\n",
    "   if dropnan:\n",
    "       agg.dropna(inplace=True)\n",
    "   return agg.values\n",
    "\n",
    "# This function will split a dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "   return data[:-n_test, :], data[-n_test:, :]\n",
    "\n",
    "# This function will fit a random forest model and make a one-step prediction\n",
    "def random_forest_forecast(train, testX):\n",
    "   train = np.array(train)\n",
    "   trainX, trainy = train[:, :-1], train[:, -1]\n",
    "   model = RandomForestRegressor(n_estimators=100)\n",
    "   model.fit(trainX, trainy)\n",
    "   yhat = model.predict([testX])\n",
    "   return yhat[0]\n",
    "\n",
    "\n",
    "# This funtion we will use for walk forward validation\n",
    "def walk_forward_validation(data, n_test):\n",
    "   predictions = list()\n",
    "   train, test = train_test_split(data, n_test)\n",
    "   history = [x for x in train]\n",
    "   for i in range(len(test)):\n",
    "       testX, testy = test[i, :-1], test[i, -1]\n",
    "       yhat = random_forest_forecast(history, testX)\n",
    "       predictions.append(yhat)\n",
    "       history.append(test[i])\n",
    "       print('>expected=%.4f, predicted=%.4f' % (testy, yhat))\n",
    "   error = mean_absolute_error(test[:, -1], predictions)\n",
    "   return error, test[:, -1], predictions\n",
    "\n",
    "# Now, we will transform the time series data into supervised learning\n",
    "values = features_array.reshape(-1, 1)\n",
    "features_array = series_to_supervised(values, n_in=12)\n",
    "\n",
    "# Let’s evaluate the model using walk-forward validation\n",
    "mae, y, yhat = walk_forward_validation(features_array, 12)\n",
    "print('MAE: %.3f' % mae)\n",
    "print('RMSE: %.3f' % rmse)\n",
    "# Finally visualize the expected vs predicted values\n",
    "pyplot.plot(y, label='Expected')\n",
    "pyplot.plot(yhat, label='Predicted')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again create a features array\n",
    "features_array = features4.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now scale the features array using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "features_array = scaler.fit_transform(features_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun with scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we will transform the time series data into supervised learning\n",
    "values = features_array.reshape(-1, 1)\n",
    "features_array = series_to_supervised(values, n_in=12)\n",
    "\n",
    "# Let’s evaluate the model using walk-forward validation\n",
    "mae, y, yhat = walk_forward_validation(features_array, 12)\n",
    "print('MAE: %.3f' % mae)\n",
    "print('RMSE: %.3f' % rmse)\n",
    "# Finally visualize the expected vs predicted values\n",
    "pyplot.plot(y, label='Expected')\n",
    "pyplot.plot(yhat, label='Predicted')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform a Random Forest Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the features3 data into target and features\n",
    "X = features3[['vehicle_count_automated_traffic_volume_counts','average_percipitation','avg_temp','vc_rating', 'ap_rating', 'at_rating','Year','Month','Is_quarter_start','Is_year_start','Elapsed']]\n",
    "y = features3['wait_assessment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the train_test_split function\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the random forest model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Mean Absolute Error: {mae:.3f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs. Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_test, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Assuming X_train is a DataFrame\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Get the unique feature names and their corresponding importances\n",
    "unique_feature_names, unique_importances = [], []\n",
    "\n",
    "for name in set(feature_names):\n",
    "    indices = [i for i, x in enumerate(feature_names) if x == name]\n",
    "    unique_feature_names.append(name)\n",
    "    unique_importances.append(sum(feature_importances[indices]))\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "unique_feature_names = np.array(unique_feature_names)\n",
    "unique_importances = np.array(unique_importances)\n",
    "\n",
    "# Get the top ten indices\n",
    "top_ten_idx = np.argsort(unique_importances)[-10:]\n",
    "\n",
    "# Create a bar plot of the top ten feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('Top 10 Feature Importances (Aggregated)')\n",
    "plt.barh(range(len(top_ten_idx)), unique_importances[top_ten_idx], align='center')\n",
    "plt.yticks(range(len(top_ten_idx)), unique_feature_names[top_ten_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a StandardScaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardize the variables in the features2 dataframe\n",
    "features3_scaled = scaler.fit_transform(features3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with the standardized variables\n",
    "features3_standardized = pd.DataFrame(features3_scaled, columns = features3.columns)\n",
    "\n",
    "# Print the first few rows of the standardized dataframe\n",
    "print(features3_standardized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features3_standardized.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the features3 data into target and features\n",
    "X = features3_standardized[['vehicle_count_automated_traffic_volume_counts','average_percipitation','avg_temp','vc_rating', 'ap_rating', 'at_rating','Year','Month','Is_quarter_start','Is_year_start','Elapsed']]\n",
    "y = features3_standardized['wait_assessment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the random forest model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs. Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_test, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Assuming X_train is a DataFrame\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Get the unique feature names and their corresponding importances\n",
    "unique_feature_names, unique_importances = [], []\n",
    "\n",
    "for name in set(feature_names):\n",
    "    indices = [i for i, x in enumerate(feature_names) if x == name]\n",
    "    unique_feature_names.append(name)\n",
    "    unique_importances.append(sum(feature_importances[indices]))\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "unique_feature_names = np.array(unique_feature_names)\n",
    "unique_importances = np.array(unique_importances)\n",
    "\n",
    "# Get the top ten indices\n",
    "top_ten_idx = np.argsort(unique_importances)[-10:]\n",
    "\n",
    "# Create a bar plot of the top ten feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('Top 10 Feature Importances (Aggregated)')\n",
    "plt.barh(range(len(top_ten_idx)), unique_importances[top_ten_idx], align='center')\n",
    "plt.yticks(range(len(top_ten_idx)), unique_feature_names[top_ten_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the features3 data into target and features\n",
    "X = features3_standardized[['vehicle_count_automated_traffic_volume_counts','average_percipitation','avg_temp','vc_rating', 'ap_rating', 'at_rating','Year','Month','Is_quarter_start','Is_year_start','Elapsed']]\n",
    "y = features3_standardized['wait_assessment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [None,2, 5, 10],\n",
    "    'min_samples_leaf': [None,1, 2, 4],\n",
    "    'max_features': [None, 'auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create the random forest model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_params)\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the features3 data into target and features\n",
    "X = features3_standardized[['vehicle_count_automated_traffic_volume_counts','average_percipitation','avg_temp','vc_rating', 'ap_rating', 'at_rating','Year','Month','Is_quarter_start','Is_year_start','Elapsed']]\n",
    "y = features3_standardized['wait_assessment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the random forest model with specified hyperparameters\n",
    "model = RandomForestRegressor(\n",
    "    max_depth=None,\n",
    "    max_features='sqrt',\n",
    "    min_samples_leaf=1,\n",
    "    min_samples_split=5,\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs. Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_test, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Assuming X_train is a DataFrame\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Get the unique feature names and their corresponding importances\n",
    "unique_feature_names, unique_importances = [], []\n",
    "\n",
    "for name in set(feature_names):\n",
    "    indices = [i for i, x in enumerate(feature_names) if x == name]\n",
    "    unique_feature_names.append(name)\n",
    "    unique_importances.append(sum(feature_importances[indices]))\n",
    "\n",
    "# Convert to NumPy arrays\n",
    "unique_feature_names = np.array(unique_feature_names)\n",
    "unique_importances = np.array(unique_importances)\n",
    "\n",
    "# Get the top ten indices\n",
    "top_ten_idx = np.argsort(unique_importances)[-10:]\n",
    "\n",
    "# Create a bar plot of the top ten feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('Top 10 Feature Importances (Aggregated)')\n",
    "plt.barh(range(len(top_ten_idx)), unique_importances[top_ten_idx], align='center')\n",
    "plt.yticks(range(len(top_ten_idx)), unique_feature_names[top_ten_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Hybrid Model using the Random Forest and Sarimax Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the Residuals and append to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features3_standardized.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals = results1.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the residuals\n",
    "plt.figure(figsize=(15, 7))\n",
    "plt.plot(residuals, color=\"blue\")\n",
    "plt.title(\"Residuals Plot\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the indices\n",
    "print(residuals.index)\n",
    "print(features3.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the index to a datetime index starting from 2015-01-01\n",
    "features3.index = pd.date_range(start='2015-01-01', freq='M', periods=len(features3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add residuals to the DataFrame\n",
    "features3['sarimax_residuals'] = residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features3.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the numerical features in the features3 dataframe using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Standardize the variables in the features2 dataframe\n",
    "features3_scaled = scaler.fit_transform(features3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dataframe with the standardized variables\n",
    "features3_standardized = pd.DataFrame(features3_scaled, columns = features3.columns)\n",
    "\n",
    "# Print the first few rows of the standardized dataframe\n",
    "print(features3_standardized.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the index to a datetime index starting from 2015-01-01\n",
    "features3_standardized.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a Random Forest Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the features3 data into target and features\n",
    "X = features3_standardized[['vehicle_count_automated_traffic_volume_counts','average_percipitation','avg_temp','vc_rating', 'ap_rating', 'at_rating','Year','Month','Is_quarter_start','Is_year_start','Elapsed', 'sarimax_residuals']]\n",
    "y = features3_standardized['wait_assessment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the random forest model with specified hyperparameters\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual vs. predicted values\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs. Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the residuals\n",
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_test, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Get the top ten indices\n",
    "top_ten_idx = np.argsort(feature_importances)[-10:]\n",
    "\n",
    "# Create a bar plot of the top ten feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.barh(range(len(top_ten_idx)), feature_importances[top_ten_idx], align='center')\n",
    "plt.yticks(range(len(top_ten_idx)), feature_names[top_ten_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform Parameter Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 5, 10],\n",
    "    'min_samples_split': [None, 2, 5, 10],\n",
    "    'min_samples_leaf': [None, 1, 2, 4],\n",
    "    'max_features': [None, 'auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "# Create the random forest model\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Perform grid search to find the best hyperparameters\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='neg_mean_absolute_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and model\n",
    "best_params = grid_search.best_params_\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set using the best model\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "# Print the best hyperparameters and evaluation metrics\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_params)\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rerun the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create the random forest model with specified hyperparameters\n",
    "model = RandomForestRegressor(\n",
    "    max_depth= None,\n",
    "    max_features= None,\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=2,\n",
    "    n_estimators=100,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Mean Absolute Error: {mae:.2f}\")\n",
    "print(f\"Root Mean Squared Error: {rmse:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the actual vs. predicted values\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Predicted Values\")\n",
    "plt.title(\"Actual vs. Predicted Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the residuals\n",
    "residuals = y_test - y_pred\n",
    "plt.scatter(y_test, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel(\"Actual Values\")\n",
    "plt.ylabel(\"Residuals\")\n",
    "plt.title(\"Residual Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "\n",
    "# Get the top ten indices\n",
    "top_ten_idx = np.argsort(feature_importances)[-10:]\n",
    "\n",
    "# Create a bar plot of the top ten feature importances\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('Top 10 Feature Importances')\n",
    "plt.barh(range(len(top_ten_idx)), feature_importances[top_ten_idx], align='center')\n",
    "plt.yticks(range(len(top_ten_idx)), feature_names[top_ten_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Multiple Route Models and Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take the Delay Data needed for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe the delays dataframe\n",
    "delays.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Join the weather_features and traffic_features dataframes together\n",
    "weather_traffic_features = pd.merge(weather_features, traffic_features, on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the date column to a datetime data type\n",
    "delays['date'] = pd.to_datetime(delays['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the date column to a datetime format\n",
    "weather_traffic_features['date'] = pd.to_datetime(weather_traffic_features['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Join the delays and weather_traffic_features dataframes together\n",
    "delays_features = pd.merge(delays, weather_traffic_features, on=\"date\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape of the weather_traffic_features dataframe\n",
    "print(delays_features.shape)\n",
    "\n",
    "# Describe the weather_traffic_features dataframe\n",
    "delays_features.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Print a value count of the route_id column\n",
    "delays_features[\"route_id\"].value_counts()\n",
    "\n",
    "# List the route_id values that have 432 rows\n",
    "delays_features[\"route_id\"].value_counts().index[delays_features[\"route_id\"].value_counts() == 432]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a route specific dataframe for the BX26 route\n",
    "delays_bx26 = delays_features[delays_features[\"route_id\"] == \"BX26\"]\n",
    "\n",
    "delays_bx26.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of the route_id values that have 432 rows\n",
    "route_ids = delays_features[\"route_id\"].value_counts().index[delays_features[\"route_id\"].value_counts() == 432].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find Valid Combinations of Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dataframe with only the route_id values that have 432 rows based on the route_ids list\n",
    "delays_features_432 = delays_features[delays_features[\"route_id\"].isin(route_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the date column to a datetime data type\n",
    "delays_features_432[\"date\"] = pd.to_datetime(delays_features_432[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Group the data by the route_id and date columns and take the mean of the wait_assessment column\n",
    "delays_features_432 = delays_features_432.groupby([\"route_id\", \"date\"]).agg({\"wait_assessment\": \"mean\", \"average_percipitation\": \"mean\", \"avg_temp\": \"mean\",\"vehicle_count_automated_traffic_volume_counts\": \"mean\"}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Create a list of the route_id values that don't have 108 rows\n",
    "route_ids2 = delays_features_432[\"route_id\"].value_counts().index[delays_features_432[\"route_id\"].value_counts() != 108].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "route_ids2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Remove the route_id values that don't have 108 rows\n",
    "delays_features_432 = delays_features_432[~delays_features_432[\"route_id\"].isin(route_ids2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the date column to a datetime data type\n",
    "delays_features_432[\"date\"] = pd.to_datetime(delays_features_432[\"date\"])\n",
    "\n",
    "# Set the date column as the index\n",
    "delays_features_432 = delays_features_432.set_index(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "filtered_df = delays_features_432"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Calculate the size of each group\n",
    "group_sizes = filtered_df.groupby(['route_id']).size().reset_index(name='group_size')\n",
    "\n",
    "# Filter groupings with 108 rows\n",
    "valid_groups = group_sizes[group_sizes['group_size'] == 108]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Merge with the original DataFrame to get the rows from valid groups\n",
    "filtered_df = pd.merge(filtered_df, valid_groups[['route_id']], on=['route_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Get a list of every route id\n",
    "route_ids = filtered_df['route_id'].unique()\n",
    "\n",
    "# Create a rolling mean diff column for each route id to ensure the time series is stationary\n",
    "for route_id in route_ids:\n",
    "    data = filtered_df[filtered_df['route_id'] == route_id].copy()  # Make a copy to avoid modifying the original DataFrame\n",
    "    rolling_mean = data['wait_assessment'].rolling(window=12, min_periods=1).mean()  # Calculate rolling mean with min_periods=1 to handle NaNs\n",
    "    data['rolling_mean_diff'] = rolling_mean.diff()  # Calculate the difference between consecutive rolling mean values\n",
    "    data['rolling_mean_diff'] = data['rolling_mean_diff'].fillna(data['rolling_mean_diff'].mean())\n",
    "    filtered_df.loc[filtered_df['route_id'] == route_id, 'rolling_mean_diff'] = data['rolling_mean_diff']  # Assign the result back to the original DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Scale the numerical columns in the filtered_df dataframe\n",
    "scaler = StandardScaler()\n",
    "filtered_df[['wait_assessment', 'average_percipitation', 'avg_temp', 'vehicle_count_automated_traffic_volume_counts', 'rolling_mean_diff']] = scaler.fit_transform(filtered_df[['wait_assessment', 'average_percipitation', 'avg_temp', 'vehicle_count_automated_traffic_volume_counts', 'rolling_mean_diff']])\n",
    "filtered_df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in the missing values with the mean of the column\n",
    "filtered_df = filtered_df.fillna(filtered_df.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the Model Looping through the Valid Combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "1"
    }
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to store the models, predictions, and route_ids\n",
    "sarimax_results = {}\n",
    "\n",
    "# Iterate over the unique route_ids\n",
    "for route_id in filtered_df['route_id'].unique():\n",
    "    # Filter the data for the current route_id\n",
    "    route_data = filtered_df[filtered_df['route_id'] == route_id]\n",
    "    \n",
    "    # Set the frequency of the index to monthly\n",
    "    route_data.index = pd.date_range(start='2015-01-01', periods=len(route_data), freq='M')\n",
    "    \n",
    "    # Define the endogenous variable (target)\n",
    "    endog = route_data['rolling_mean_diff']\n",
    "\n",
    "    # Define the exogenous variables\n",
    "    exog = route_data[['average_percipitation', 'avg_temp', 'vehicle_count_automated_traffic_volume_counts']]\n",
    "    \n",
    "    # Add a constant to the exogenous variables\n",
    "    exog = sm.add_constant(exog)\n",
    "    \n",
    "    # Perform test/train split\n",
    "    train_size = 0.8  # You can adjust this ratio as needed\n",
    "    endog_train, endog_test, exog_train, exog_test = train_test_split(\n",
    "    endog, exog, train_size=train_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    # Set the frequency of the time series to monthly\n",
    "    freq = 'M'\n",
    "    \n",
    "    # Create the SARIMAX model\n",
    "    model = sm.tsa.SARIMAX(endog_train, exog = exog_train, order=(0, 1, 0), seasonal_order=(0, 1, 1, 12), freq = freq)\n",
    "    \n",
    "    # Fit the model\n",
    "    model_fit = model.fit()\n",
    "    \n",
    "    # Make predictions on the test set\n",
    "    forecast = model_fit.get_forecast(steps=len(endog_test), exog=exog_test)\n",
    "    \n",
    "    # Save the model, predictions, and route_id to the dictionary\n",
    "    sarimax_results[route_id] = {'model': model_fit, 'predictions': forecast, 'route_id': route_id}\n",
    "\n",
    "# Print the dictionary of models, predictions, and route_ids\n",
    "print(sarimax_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_data.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create dashboard to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dropdown with unique route_ids\n",
    "route_id_options = [{'label': str(route_id), 'value': route_id} for route_id in sarimax_results.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Dash app\n",
    "app = dash.Dash(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Layout of the app\n",
    "app.layout = html.Div([\n",
    "    html.Label('Select Route Id:', style={'font-weight': 'bold', 'margin-bottom': '10px'}),\n",
    "    dcc.Dropdown(\n",
    "        id='route-id-dropdown',\n",
    "        options=route_id_options,\n",
    "        value=route_id_options[0]['value'],\n",
    "        style={'width': '50%', 'margin-bottom': '20px'}\n",
    "    ),\n",
    "    dcc.Graph(id='time-series-plot',\n",
    "        config={'displayModeBar': False})\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callback to update the plot based on the selected Route Id\n",
    "@app.callback(\n",
    "    dash.dependencies.Output('time-series-plot', 'figure'),\n",
    "    [dash.dependencies.Input('route-id-dropdown', 'value')]\n",
    ")\n",
    "def update_plot(selected_route_id):\n",
    "    # Retrieve model and predictions for the selected Route Id\n",
    "    model_fit = sarimax_results[selected_route_id]['model']\n",
    "    predictions = sarimax_results[selected_route_id]['predictions']\n",
    "    route_data = filtered_df[filtered_df['route_id'] == selected_route_id]\n",
    "    endog_train = route_data['rolling_mean_diff'][:int(len(route_data) * 0.8)]\n",
    "    endog_test = route_data['rolling_mean_diff'][int(len(route_data) * 0.8):]\n",
    "\n",
    "    # Plot actual values from the training set\n",
    "    trace1 = go.Scatter(x=endog_train.index, y=endog_train, mode='lines', name='Actual (Training Set)', line=dict(color='blue'))\n",
    "\n",
    "    # Plot actual values from the test set\n",
    "    trace2 = go.Scatter(x=endog_test.index, y=endog_test, mode='lines', name='Actual (Test Set)', line=dict(color='blue', dash='dash'))\n",
    "\n",
    "    # Plot predicted values from the test set\n",
    "    trace3 = go.Scatter(x=endog_test.index, y=predictions.predicted_mean.to_numpy(), mode='lines', name='Predicted (Test Set)', line=dict(color='orange'))\n",
    "\n",
    "    # Set layout options\n",
    "    layout = dict(\n",
    "        title='Actual vs Predicted Values',\n",
    "        xaxis=dict(title='Date'),\n",
    "        yaxis=dict(title='Wait Assessment'),\n",
    "        legend=dict(x=0, y=1, traceorder='normal')\n",
    "    )\n",
    "\n",
    "    # Create the figure\n",
    "    figure = dict(data=[trace1, trace2, trace3], layout=layout)\n",
    "\n",
    "    return figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the app\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True, use_reloader=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
